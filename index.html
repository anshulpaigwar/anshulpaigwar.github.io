<!DOCTYPE HTML>
<html lang="en">

<head>
  <meta http-equiv="Content-Type" content="text/html; charset=UTF-8">

  <title>Anshul Paigwar</title>

  <meta name="author" content="Anshul Paigwar">
  <meta name="viewport" content="width=device-width, initial-scale=1">
  <meta name="google-site-verification" content="6qogr8z7RZgR_SCX2GSXZNaoOxokGl59tYYreLe6uEg" />
  <meta property="og:image" content="images/Anshul_Paigwar.jpg" />
  <meta name="og:title" content="Anshul Paigwar" />
  <meta name="title" content="Anshul Paigwar" />
  <meta name="keywords"
    content="Anshul Paigwar, Anshul, Paigwar, Resume, CV, Delice Robotics, Robodelice, Ivlabs, Frustrum PointPillars, Inria, GndNet">
  <meta name="og:description"
    content="Anshul Paigwar is a research engineer at Inria-Grenoble, France, where he work on computer vision and deep learning." />
  <meta name="description"
    content="Anshul Paigwar is a research engineer at Inria-Grenoble, France, where he work on computer vision and deep learning."">
  <link rel=" stylesheet" type="text/css" href="stylesheet.css">
  <link rel="icon"
    href="data:image/svg+xml,<svg xmlns=%22http://www.w3.org/2000/svg%22 viewBox=%220 0 100 100%22><text y=%22.9em%22 font-size=%2290%22>üåê</text></svg>">
</head>


<body>
  <table
    style="width:100%;max-width:800px;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;">
    <tbody>
      <tr style="padding:0px">
        <td style="padding:0px">
          <table
            style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;">
            <tbody>
              <tr style="padding:0px">
                <td style="padding:2.5%;width:63%;vertical-align:middle">
                  <p style="text-align:center">
                    <name>Anshul Paigwar</name>
                  <p>I am a research engineer at team <a href="https://team.inria.fr/chroma/en/">Chroma</a> in <a
                      href="https://www.inria.fr/en">Inria</a> Grenoble, France, where I work on computer vision and
                    deep learning.
                  </p>
                  <p>
                    At Inria, I am supervised by Prof. <a
                      href="https://scholar.google.com/citations?user=jtnM0c8AAAAJ&hl=en">Christian Laugier</a>,
                    mainly focusing on developing robust and real-time perception systems for autonomous vehicles. I
                    deal with large-scale data streams from RGB cameras,
                    LiDARs, and Event-based cameras. I've worked on sensor fusion techniques for 2D and 3D object
                    detection, semantic segmentation, depth estimation,
                    motion forecasting, occupancy grid prediction, collision risk estimation, simulations in CARLA and
                    Gazebo, localization, tracking, and many more applications.

                    <!-- <a href="https://ai.googleblog.com/2020/12/portrait-light-enhancing-portrait.html">Portrait Light</a>, <a href="https://ai.googleblog.com/2014/04/lens-blur-in-new-google-camera-app.html">Lens Blur</a>, <a href="https://ai.googleblog.com/2014/10/hdr-low-light-and-high-dynamic-range.html">HDR+</a>, <a href="https://www.google.com/get/cardboard/jump/">Jump</a>, <a href="https://ai.googleblog.com/2017/10/portrait-mode-on-pixel-2-and-pixel-2-xl.html">Portrait Mode</a>, <a href="https://www.youtube.com/watch?v=JSnB06um5r4">Glass</a>, and <a href="https://www.matthewtancik.com/nerf">NeRF</a>. I did my PhD at <a href="http://www.eecs.berkeley.edu/">UC Berkeley</a>, where I was advised by <a href="http://www.cs.berkeley.edu/~malik/">Jitendra Malik</a> and funded by the <a href="http://www.nsfgrfp.org/">NSF GRFP</a>. I've received the <a href="https://www2.eecs.berkeley.edu/Students/Awards/15/">C.V. Ramamoorthy Distinguished Research Award</a> and the <a href="https://www.thecvf.com/?page_id=413#YRA">PAMI Young Researcher Award</a>. -->
                  </p>

                  <p>
                    I got my master's in Graphics, Vision, and Robotics from <a
                      href="https://www.wikiwand.com/en/%C3%89cole_nationale_sup%C3%A9rieure_d%27informatique_et_de_math%C3%A9matiques_appliqu%C3%A9es_de_Grenoble">ENSIMAG</a>,
                    a prestigious French Grande √âcole in Grenoble, France.
                    My master thesis was supervised by Prof. <a href="https://chriswolfvision.github.io/www/">Christian
                      Wolf</a>, on exploring visual attention mechanisms for 3D space.
                  </p>
                  <p>I am an active reviewer at top robotics conferences including ICRA, IROS, IV, ITSC, ICARCV,
                    CIS-RAM.</p>


                  <p style="text-align:center">
                    <a href="mailto:p.anshul6@gmail.com">Email</a> &nbsp/&nbsp
                    <a href="data/Anshul_Paigwar_CV.pdf">CV</a> &nbsp/&nbsp
                    <!-- <a href="data/JonBarron-bio.txt">Bio</a> &nbsp/&nbsp -->
                    <a href="https://scholar.google.com/citations?user=V2yrxx4AAAAJ&hl=en">Google Scholar</a>
                    &nbsp/&nbsp
                    <a href="https://github.com/anshulpaigwar/">Github</a> &nbsp/&nbsp
                    <a href="http://anshulpaigwar.weebly.com/">Website</a> &nbsp/&nbsp
                    <a href="https://medium.com/@anshulpaigwar">Medium</a>
                  </p>
                </td>
                <td style="padding:2.5%;width:40%;max-width:40%">
                  <a href="images/Anshul_Paigwar_circle.jpg"><img style="width:100%;max-width:100%" alt="profile photo"
                      src="images/Anshul_Paigwar_circle.jpg" class="hoverZoomLink"></a>
                </td>
              </tr>
            </tbody>
          </table>




          <table
            style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;">
            <tbody>
              <tr>
                <td style="padding:20px;width:100%;vertical-align:middle">
                  <heading>Leadership</heading>
                  <p>Along with my research interests, I hold a strong entrepreneurial spirit, I founded
                    <a href="https://delicerobotics.com/">Delice Robotics</a> and helped kickstart <a
                      href="https://www.ivlabs.in/">Ivlabs</a>.
                  </p>
                </td>
              </tr>
            </tbody>
          </table>
          <table
            style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;">
            <tbody>





              <tr onmouseout="delice_robotics_stop()" onmouseover="delice_robotics_start()">
                <td style="padding:20px;width:30%;vertical-align:middle">
                  <div class="one">
                    <div class="hidden" id='delice_robotics_image'><video width=100% height=100% muted autoplay loop>
                        <source src="images/delice_robotics_video.mp4" type="video/mp4">
                        Your browser does not support the video tag.
                      </video></div>
                    <div id='delice_robotics_still'><img src='images/delice_robotics_still.jpg' width=100%></div>
                  </div>
                  <script type="text/javascript">
                    function delice_robotics_start() {
                      document.getElementById('delice_robotics_image').style.display = 'inline';
                      document.getElementById('delice_robotics_still').style.display = 'none';
                    }

                    function delice_robotics_stop() {
                      document.getElementById('delice_robotics_image').style.display = 'none';
                      document.getElementById('delice_robotics_still').style.display = 'inline';
                    }
                    delice_robotics_stop()
                  </script>
                </td>



                <td style="padding:20px;width:70%;vertical-align:middle">
                  <a href="https://delicerobotics.com/">
                    <papertitle>Delice Robotics - Future of food preparation
                    </papertitle>
                  </a>
                  <br>
                  <strong>Anshul Paigwar</strong>,
                  <a href="https://www.linkedin.com/in/yoann-allardin/?originalSubdomain=fr">Yoann Allardin</a>,
                  <a href="https://www.linkedin.com/in/didier-lasserre-pro/">Didier Lasserre</a>
                  <a href="https://fr.linkedin.com/in/samuel-heidmann-55064094">Samuel Heidmann</a>

                  <br>
                  <em>Incubated at Inria Startup studio, France, Nov 2020 - Nov 2021 </em>
                  <br>
                  <a href="https://delicerobotics.com/">website</a>
                  <!-- /
                <a href="https://github.com/anshulpaigwar/Frustum-Pointpillars">code</a>
                /
                <a href="https://www.youtube.com/watch?v=0z7OPPRsqTk">presentation video</a>
                /
                <a href="https://www.youtube.com/watch?v=mJyKwhOOqxU">results video</a> -->

                  <p>At D√©lice we built Robotic and AI systems to automate food preparation and vending. The
                    aim was to make quality food more affordable and available to everyone. I worked on
                    multiple facets including project ideation, fundraising (100K euros), team building,
                    prototyping, business model, customer need understanding, product demos & presentation.</p>
                </td>
              </tr>





              <tr onmouseout="ivlabs_stop()" onmouseover="ivlabs_start()">
                <td style="padding:20px;width:30%;vertical-align:middle">
                  <div class="one">
                    <div class="hidden" id='ivlabs_image'><video width=100% height=100% muted autoplay loop>
                        <source src="images/ivlabs_video.mp4" type="video/mp4">
                        Your browser does not support the video tag.
                      </video></div>
                    <div id='ivlabs_still'><img src='images/ivlabs_still.png' width=100%></div>
                  </div>
                  <script type="text/javascript">
                    function ivlabs_start() {
                      document.getElementById('ivlabs_image').style.display = 'none';
                      document.getElementById('ivlabs_still').style.display = 'inline';
                    }

                    function ivlabs_stop() {
                      document.getElementById('ivlabs_image').style.display = 'none';
                      document.getElementById('ivlabs_still').style.display = 'inline';
                    }
                    ivlabs_stop()
                  </script>
                </td>


                <td style="padding:20px;width:70%;vertical-align:middle">
                  <a href="https://www.ivlabs.in/">
                    <papertitle> Ivlabs - Inspiring Innovation
                    </papertitle>
                  </a>
                  <br>
                  <a href="https://scholar.google.co.in/citations?user=hUnCf2YAAAAJ&hl=en">Rohan Thakker</a>,
                  <a href="https://www.ajinkyakamat.com/home">Ajinkya Kamat</a>,
                  <strong>Anshul Paigwar</strong>,
                  <a href="https://in.linkedin.com/in/saiteja-manchukanti">Sai Teja Manchukanti</a>,
                  <a href="https://scholar.google.it/citations?user=-AbpUDEAAAAJ&hl=en">Akash Singh</a>,
                  <a href="https://prasadvagdargi.github.io/">Prasad Vagdargi</a>,
                  <a href="https://www.linkedin.com/in/manish-the-maker">Manish Maurya</a>,
                  <a href="https://sites.google.com/view/manishsaroya">Manish Saroya</a>, and with many other great
                  minds.

                  <br>
                  <em>Student's Robotics and AI lab at VNIT, Nagpur India </em>
                  <br>
                  <a href="https://www.ivlabs.in/">website</a>
                  /
                  <a href="https://www.ivlabs.in/publications.html">publications</a>
                  /
                  <a href="https://www.youtube.com/channel/UC2Ud1ZH23JJvmnb6aHCu51g/videos">project videos</a>
                  /
                  <a href="https://github.com/IvLabs">code base</a>

                  <p>I helped kickstart IvLabs! It has grown to be among the top robotics labs in India with over 100
                    active members.
                    I regularly mentor and manage student projects at IvLabs as my responsibility to impart knowledge
                    and give back
                    to the community.</p>
                </td>
              </tr>


            </tbody>
          </table>








          <table
            style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;">
            <tbody>
              <tr>
                <td style="padding:20px;width:100%;vertical-align:middle">
                  <heading>Research</heading>
                  <p>
                    I'm interested in computer vision, machine learning, and image processing. Much of my research is
                    about inferring the
                    dynamic environment around robot from images and point cloud. Representative papers are <span
                      class="highlight">highlighted</span>.
                  </p>
                </td>
              </tr>
            </tbody>
          </table>
          <table
            style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;">
            <tbody>





              <!-- PAPER ITSC 2022 -->
              <!-- <tr onmouseout="itsc_2022_stop()" onmouseover="itsc_2022_start()">
                <td style="padding:20px;width:30%;vertical-align:middle">
                  <div class="one">
                    <div class="two" id='itsc_2022_image'><video width=100% height=100% muted autoplay loop>
                        <source src="images/ITSC22_video.mp4" type="video/mp4">
                        Your browser does not support the video tag.
                      </video></div>
                    <img src='images/ITSC22_still.png' width=100%>
                  </div>
                  <script type="text/javascript">
                    function itsc_2022_start() {
                      document.getElementById('itsc_2022_image').style.opacity = "1";
                    }

                    function itsc_2022_stop() {
                      document.getElementById('itsc_2022_image').style.opacity = "0";
                    }
                    itsc_2022_stop()
                  </script>
                </td> -->



              <tr onmouseout="itsc_2022_stop()" onmouseover="itsc_2022_start()">
                <td style="padding:20px;width:30%;vertical-align:middle">
                  <div class="one">
                    <div class="two" id='itsc_2022_video'><video width=100% muted autoplay loop>
                        <source src="images/ITSC22_video.mp4" type="video/mp4">
                        Your browser does not support the video tag.
                      </video></div>
                    <div id='itsc_2022_still'><img src='images/ITSC22_still.png' width=100%></div>
                  </div>
                  <script type="text/javascript">
                    function itsc_2022_start() {
                      document.getElementById('itsc_2022_video').style.display = 'inline';
                      document.getElementById('itsc_2022_still').style.display = 'none';
                    }

                    function itsc_2022_stop() {
                      document.getElementById('itsc_2022_video').style.display = 'none';
                      document.getElementById('itsc_2022_still').style.display = 'inline';
                    }
                    itsc_2022_stop()
                  </script>
                </td>



                <td
                  style="padding-left:20px;padding-right: 20px; padding-top:10px;padding-bottom: 10px;width:75%;vertical-align:middle">
                  <a href="https://github.com/d451gon/MULTILANE">
                    <papertitle>MultiLane: Lane intention prediction and sensible lane-oriented trajectory forecasting
                      on centerline
                      graphs</papertitle>
                  </a>
                  <br>
                  <a href="https://www.davidsgonzalez.com/">David Sierra-Gonzalez</a>,
                  <strong>Anshul Paigwar</strong>,
                  <a href="https://scholar.google.com/citations?user=5QMAbisAAAAJ&hl=en">Ozgur Erkent</a>,
                  <a href="https://scholar.google.com/citations?user=jtnM0c8AAAAJ&hl=en">Christian Laugier</a>
                  <br>
                  <em>ITSC</em>, 2022. <em>To appear.</em>
                  <br>
                  <a href="https://github.com/d451gon/MULTILANE">code</a>
                  <p></p>
                  <p>A Graph Neural Network is used to predict the centerline that the target intends to follow. Based
                    on that, we
                    predict a distribution over potential endpoints, and multiple lane-oriented trajectory realizations.
                  </p>
                </td>
              </tr>
              <!-- END PAPER ITSC 2022 -->






              <!-- PAPER ICARCV 2022 -->

              <tr onmouseout="icarcv_2022_stop()" onmouseover="icarcv_2022_start()">
                <td style="padding:20px;width:30%;vertical-align:middle">
                  <div class="one">

                    <div class="two" id='icarcv_2022_video'><video width=100% muted autoplay loop>
                        <source src="images/ICARCV22_video.mp4" type="video/mp4">
                        Your browser does not support the video tag.
                      </video></div>

                    <div id='icarcv_2022_still'><img src='images/ICARCV22_still.png' width=100%></div>
                  </div>
                  <script type="text/javascript">
                    function icarcv_2022_start() {
                      document.getElementById('icarcv_2022_video').style.display = 'inline';
                      document.getElementById('icarcv_2022_still').style.display = 'none';
                    }

                    function icarcv_2022_stop() {
                      document.getElementById('icarcv_2022_video').style.display = 'none';
                      document.getElementById('icarcv_2022_still').style.display = 'inline';
                    }
                    icarcv_2022_stop()
                  </script>
                </td>




                <td
                  style="padding-left:20px;padding-right: 20px; padding-top:10px;padding-bottom: 10px;width:75%;vertical-align:middle">
                  <a href="">
                    <papertitle>TransFuseGrid: Transformer-based Lidar-RGB fusion for semantic grid prediction
                    </papertitle>
                  </a>
                  <br>
                  <a href="https://github.com/gsg213">Gustavo Salazar-Gomez</a>,
                  <strong>Anshul Paigwar</strong>,
                  <a href="https://scholar.google.com/citations?user=6yh4f4YAAAAJ&hl=en">Wenqian Liu</a>,
                  <a href="https://scholar.google.com/citations?user=5QMAbisAAAAJ&hl=en">Ozgur Erkent</a>,
                  <a href="https://scholar.google.com/citations?user=hM_mm8MAAAAJ&hl=en">Manuel Diaz</a>,
                  <a href="https://www.davidsgonzalez.com/">David Sierra-Gonzalez</a>,
                  <a href="https://scholar.google.com/citations?user=jtnM0c8AAAAJ&hl=en">Christian Laugier</a>
                  <br>
                  <em> To appear.</em><!-- ICARCV</em>, 2022 -->
                  <br>
                  <p></p>
                  <p>Architecture that fuses multi-camera and LiDAR data at different scales to produce bird's eye view
                    semantic grids
                    of the environment.</p>
                </td>
              </tr>
              <!-- END PAPER ICARCV 2022 -->



              <!-- START PAPER ICCV21 2022 -->
              <tr onmouseout="fpp_stop()" onmouseover="fpp_start()">
                <td style="padding:20px;width:30%;vertical-align:middle">
                  <div class="one">
                    <div class="two" id='fpp_image'><video width=100% height=100% muted autoplay loop>
                        <source src="images/frustrum-pointpillars.mp4" type="video/mp4">
                        Your browser does not support the video tag.
                      </video></div>
                    <div id='fpp_still'><img src='images/frustrum-pointpillars.png' width=100%></div>
                  </div>
                  <script type="text/javascript">
                    function fpp_start() {
                      document.getElementById('fpp_image').style.display = 'inline';
                      document.getElementById('fpp_still').style.display = 'none';
                    }

                    function fpp_stop() {
                      document.getElementById('fpp_image').style.display = 'none';
                      document.getElementById('fpp_still').style.display = 'inline';
                    }
                    fpp_stop()
                  </script>
                </td>


                <td style="padding:20px;width:70%;vertical-align:middle">
                  <a href="https://hal.archives-ouvertes.fr/hal-03354114/">
                    <papertitle>Frustrum-Pointpillars: A Multi-Stage Approach for 3D Object Detection using RGB Camera
                      and LiDAR</papertitle>
                  </a>
                  <br>
                  <strong>Anshul Paigwar</strong>,
                  <a href="https://www.davidsgonzalez.com/">David Sierra-Gonzalez</a>,
                  <a href="https://oerkent.gitlabpages.inria.fr/ozgurerkent/index.html">√ñzg√ºr Erkent</a>,
                  <a href="https://scholar.google.com/citations?user=jtnM0c8AAAAJ&hl=en">Christian Laugier</a>

                  <br>
                  <em>IEEE/CVF International Conference on Computer Vision (ICCV)</em>, 2021, Workshop on Autonomous
                  Vehicle Vision
                  <br>
                  <a href="https://hal.archives-ouvertes.fr/hal-03354114/">pdf</a>
                  /
                  <a href="https://github.com/anshulpaigwar/Frustum-Pointpillars">code</a>
                  /
                  <a href="https://www.youtube.com/watch?v=0z7OPPRsqTk">presentation video</a>
                  /
                  <a href="https://www.youtube.com/watch?v=mJyKwhOOqxU">results video</a>
                  <p></p>
                  <p>We leverage 2D object detection to reduce the search space in the 3D.
                    Then use the Pillar Feature Encoding network for object localization in the reduced point cloud. At
                    the time of
                    submission F-Pointpillars <strong>ranked among top 5 </strong> approches for BEV pedestrian
                    detection on KITTI Dataset.</p>
                </td>
              </tr>
              <!-- END OF PAPER ICCV21 2022 -->




              <tr onmouseout="IROS20_stop()" onmouseover="IROS20_start()">
                <td style="padding:20px;width:30%;vertical-align:middle">
                  <div class="one">
                    <div class="two" id='IROS20_video'><video width=100% height=100% muted autoplay loop>
                        <source src="images/IROS20_video.mp4" type="video/mp4">
                        Your browser does not support the video tag.
                      </video></div>
                    <div id='IROS20_still'><img src='images/IROS20_still.png' width=100%></div>
                  </div>
                  <script type="text/javascript">
                    function IROS20_start() {
                      document.getElementById('IROS20_video').style.display = 'inline';
                      document.getElementById('IROS20_still').style.display = 'none';
                    }

                    function IROS20_stop() {
                      document.getElementById('IROS20_video').style.display = 'none';
                      document.getElementById('IROS20_still').style.display = 'inline';
                    }
                    IROS20_stop()
                  </script>
                </td>


                <td style="padding:20px;width:70%;vertical-align:middle">
                  <a href="https://hal.inria.fr/hal-02927350/">
                    <papertitle>GndNet: Fast Ground plane Estimation and Point Cloud Segmentation for Autonomous
                      Vehicles
                    </papertitle>
                  </a>
                  <br>
                  <strong>Anshul Paigwar</strong>,
                  <a href="https://oerkent.gitlabpages.inria.fr/ozgurerkent/index.html">√ñzg√ºr Erkent</a>,
                  <a href="https://www.davidsgonzalez.com/">David Sierra-Gonzalez</a>,
                  <a href="https://scholar.google.com/citations?user=jtnM0c8AAAAJ&hl=en">Christian Laugier</a>

                  <br>
                  <em>IEEE International conference on Robotic Systems (IROS), 2020 </em>
                  <br>
                  <a href="https://hal.inria.fr/hal-02927350/">pdf</a>
                  /
                  <a href="https://github.com/anshulpaigwar/GndNet">code</a>
                  /
                  <a href="https://www.youtube.com/watch?v=kjZ-n_aIJAg">presentation video</a>
                  /
                  <a href="https://www.youtube.com/watch?v=W_jXU-ewJR0&t">results video</a>
                  <p></p>
                  <p>GndNet uses PointNet and Pillar Feature Encoding network to extract features and regresses ground
                    height for each cell
                    of the grid. GndNet establishes a new state-of-the-art, achieves a <strong>run-time of 55Hz
                    </strong> for ground plane estimation and ground point
                    segmentation.</p>
                </td>
              </tr>



              <tr onmouseout="CVPRW19_stop()" onmouseover="CVPRW19_start()">
                <td style="padding:20px;width:30%;vertical-align:middle">
                  <div class="one">
                    <div class="two" id='CVPRW19_video'><video width=100% height=100% muted autoplay loop>
                        <source src="images/CVPRW19_video.mp4" type="video/mp4">
                        Your browser does not support the video tag.
                      </video></div>
                    <div id='CVPRW19_still'><img src='images/CVPRW19_still.png' width=100%></div>
                  </div>
                  <script type="text/javascript">
                    function CVPRW19_start() {
                      document.getElementById('CVPRW19_video').style.display = 'inline';
                      document.getElementById('CVPRW19_still').style.display = 'none';
                    }

                    function CVPRW19_stop() {
                      document.getElementById('CVPRW19_video').style.display = 'none';
                      document.getElementById('CVPRW19_still').style.display = 'inline';
                    }
                    CVPRW19_stop()
                  </script>
                </td>


                <td style="padding:20px;width:70%;vertical-align:middle">
                  <a href="https://hal.inria.fr/hal-02156555">
                    <papertitle>Attentional PointNet for 3D-Object Detection in Point Clouds
                    </papertitle>
                  </a>
                  <br>
                  <strong>Anshul Paigwar</strong>,
                  <a href="https://oerkent.gitlabpages.inria.fr/ozgurerkent/index.html">√ñzg√ºr Erkent</a>,
                  <a href="https://chriswolfvision.github.io/www/">Christian Wolf</a>,
                  <a href="https://scholar.google.com/citations?user=jtnM0c8AAAAJ&hl=en">Christian Laugier</a>

                  <br>
                  <em>IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR), 2019, Workshop on
                    Autonomous Driving </em>
                  <br>
                  <a href="https://hal.inria.fr/hal-02156555">pdf</a>
                  /
                  <a href="https://github.com/anshulpaigwar/Attentional-PointNet">code</a>
                  /
                  <a
                    href="https://drive.google.com/file/d/1q2sOJBA6nBhDnkVPV5iiiTnuKpEAtM70/view?usp=sharing">poster</a>

                  <p>We extend the theory of visual attention mechanisms to 3D point clouds and introduce a new
                    recurrent 3D Localization
                    Network module. Rather than processing the whole point cloud, the network learns where to look
                    (finding regions of
                    interest), which significantly reduces the number of points to be processed and inference time.</p>
                </td>
              </tr>










              <tr onmouseout="icra2022_stop()" onmouseover="icra2022_start()">
                <td style="padding:20px;width:30%;vertical-align:middle">
                  <div class="one">
                    <div class="two" id='icra22_video'><video width=100% muted autoplay loop>
                        <source src="images/ICRA22_video.mp4" type="video/mp4">
                        Your browser does not support the video tag.
                      </video></div>
                    <div class="two" id='icra22_still'><img src='images/ICRA22_still.png' width=100%></div>

                  </div>
                  <script type="text/javascript">
                    function icra2022_start() {
                      document.getElementById('icra22_video').style.display = 'inline';
                      document.getElementById('icra22_still').style.display = 'none';
                    }

                    function icra2022_stop() {
                      document.getElementById('icra22_video').style.display = 'none';
                      document.getElementById('icra22_still').style.display = 'inline';
                    }
                    icra2022_stop()
                  </script>
                </td>


                <td style="padding:20px;width:70%;vertical-align:middle">
                  <a href="https://hal.archives-ouvertes.fr/hal-03591717/">
                    <papertitle>Fusing Event-based and RGB camera for Robust Object Detection in Adverse Conditions
                    </papertitle>
                  </a>
                  <br>
                  <a href="https://scholar.google.com/citations?user=Q8aj04wAAAAJ&hl=en">Abhishek Tomy</a>,
                  <strong>Anshul Paigwar</strong>,
                  <a href="https://khushdeep-singh.github.io/khushdeep.io/">Khushdeep Singh Mann</a>,
                  <a href="https://sites.google.com/site/arenzaglia/">Alessandro Renzaglia</a>,
                  <a href="https://scholar.google.com/citations?user=jtnM0c8AAAAJ&hl=en">Christian Laugier</a>

                  <br>
                  <em>IEEE International Conference on Robotics and Automation (ICRA)</em>, 2022, Philadelphia, United
                  States.
                  <br>
                  <a href="https://hal.archives-ouvertes.fr/hal-03591717/">pdf</a>
                  /
                  <a href="https://github.com/abhishek1411/event-rgb-fusion">code</a>
                  /
                  <a href="https://www.youtube.com/watch?v=2nrqhiiXJwY">results video</a>
                  <p></p>
                  <p>We propose a redundant sensor fusion model of event-based and frame-based cameras that is robust to
                    common image
                    corruptions. Our sensor fusion approach is over <strong>30% more robust to corruptions </strong>
                    compared to only frame-based detections and
                    outperforms the only event-based detection.</p>
                </td>
              </tr>




              <tr onmouseout="cvprw21_stop()" onmouseover="cvprw21_start()">
                <td style="padding:20px;width:30%;vertical-align:middle">
                  <div class="one">
                    <div class="two" id='cvprw21_video'><video width=100% height=100% muted autoplay loop>
                        <source src="images/CVPRW21_video.mp4" type="video/mp4">
                        Your browser does not support the video tag.
                      </video></div>
                    <div id='cvprw21_still'><img src='images/CVPRW21_still.png' width=100%></div>

                  </div>
                  <script type="text/javascript">
                    function cvprw21_start() {
                      document.getElementById('cvprw21_video').style.display = 'inline';
                      document.getElementById('cvprw21_still').style.display = 'none';
                    }

                    function cvprw21_stop() {
                      document.getElementById('cvprw21_video').style.display = 'none';
                      document.getElementById('cvprw21_still').style.display = 'inline';
                    }
                    cvprw21_stop()
                  </script>
                </td>


                <td style="padding:20px;width:70%;vertical-align:middle">
                  <a href="https://hal.inria.fr/hal-03368215/">
                    <papertitle>STADIE-Net: Stagewise Disparity Estimation from Stereo Event-based Cameras
                    </papertitle>
                  </a>
                  <br>
                  <a href="https://scholar.google.com/citations?user=Q8aj04wAAAAJ&hl=en">Abhishek Tomy</a>,
                  <strong>Anshul Paigwar</strong>,
                  <!-- <a href="https://khushdeep-singh.github.io/khushdeep.io/">Khushdeep Singh Mann</a>, -->
                  <a href="https://sites.google.com/site/arenzaglia/">Alessandro Renzaglia</a>,
                  <a href="https://scholar.google.com/citations?user=jtnM0c8AAAAJ&hl=en">Christian Laugier</a>

                  <br>
                  <em>IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR), 2021, Workshop on
                    Event-based Vision
                  </em>
                  <br>
                  <a href="https://hal.inria.fr/hal-03368215/">pdf</a>
                  /
                  <a href="https://github.com/abhishek1411/event-rgb-fusion">code</a>

                  <!-- <a href="https://www.youtube.com/watch?v=2nrqhiiXJwY">results video</a> -->
                  <p></p>
                  <p>STADIE-Net takes advantage of stagewise refinement and prediction of disparity using events from 2
                    neuromorphic cameras in a stereo setup. The method utilizes voxel grid representation for events as
                    input and proposes a
                    4 stage network going from coarse to finer disparity prediction.</p>
                </td>
              </tr>



              <tr onmouseout="IV22_stop()" onmouseover="IV22_start()">
                <td style="padding:20px;width:30%;vertical-align:middle">
                  <div class="one">
                    <div id='IV22_anim' class='two'>
                      <img src="images/IV22_anim.gif" width=100%></a>
                    </div>
                    <div id='IV22_still'><img src="images/IV22_still.gif" width=100%></div>
                  </div>
                  <script type="text/javascript">
                    function IV22_start() {
                      document.getElementById('IV22_anim').style.display = 'inline';
                      document.getElementById('IV22_still').style.display = 'none';
                    }

                    function IV22_stop() {
                      document.getElementById('IV22_anim').style.display = 'none';
                      document.getElementById('IV22_still').style.display = 'inline';
                    }
                    IV22_stop()
                  </script>
                </td>

                <td style="padding:20px;width:70%;vertical-align:middle">
                  <a href="https://arxiv.org/abs/2205.03212">
                    <papertitle>Predicting Future Occupancy Grids in Dynamic Environment with Spatio-Temporal Learning
                    </papertitle>
                    <!-- </a> -->
                    <br>
                    <a href="https://khushdeep-singh.github.io/khushdeep.io/">Khushdeep Singh Mann</a>,
                    <a href="https://scholar.google.com/citations?user=Q8aj04wAAAAJ&hl=en">Abhishek Tomy</a>,
                    <strong>Anshul Paigwar</strong>,
                    <a href="https://sites.google.com/site/arenzaglia/">Alessandro Renzaglia</a>,
                    <a href="https://scholar.google.com/citations?user=jtnM0c8AAAAJ&hl=en">Christian Laugier</a>

                    <br>
                    <em>IEEE Intelligent Vehicles Symposium (IV), 2022, Aachen, Germany </em>
                    <br>
                    <a href="https://arxiv.org/abs/2205.03212">pdf</a>

                    <!-- <a href="https://github.com/abhishek1411/event-rgb-fusion">code</a> -->

                    <!-- <a href="https://www.youtube.com/watch?v=2nrqhiiXJwY">results video</a> -->
                    <p></p>
                    <p> We propose a spatio-temporal prediction network pipeline that takes the past information from
                      the
                      environment and
                      semantic labels separately to predict occupancy for a longer horizon of 3 seconds and in a
                      relatively complex environment.</p>
                </td>
              </tr>









              <tr onmouseout="ICAR21_stop()" onmouseover="ICAR21_start()">
                <td style="padding:20px;width:30%;vertical-align:middle">
                  <div class="one">
                    <div class="two" id='ICAR21_video'><video width=100% height=100% muted autoplay loop>
                        <source src="images/ICAR21_video.mp4" type="video/mp4">
                        Your browser does not support the video tag.
                      </video></div>
                    <div id='ICAR21_still'><img src='images/ICAR21_still.png' width=100%></div>
                  </div>
                  <script type="text/javascript">
                    function ICAR21_start() {
                      document.getElementById('ICAR21_video').style.display = 'inline';
                      document.getElementById('ICAR21_still').style.display = 'none';
                    }

                    function ICAR21_stop() {
                      document.getElementById('ICAR21_video').style.display = 'none';
                      document.getElementById('ICAR21_still').style.display = 'inline';
                    }
                    ICAR21_stop()
                  </script>
                </td>


                <td style="padding:20px;width:70%;vertical-align:middle">
                  <a href="https://hal.inria.fr/hal-03416222/">
                    <papertitle>Real-time Collision Risk Estimation based on Stochastic Reachability Spaces
                    </papertitle>
                  </a>
                  <br>
                  <a href="https://scholar.google.co.in/citations?user=QN_sbHmzDugC&hl=en">Unmesh Patil</a>,
                  <a href="https://sites.google.com/site/arenzaglia/">Alessandro Renzaglia</a>,
                  <strong>Anshul Paigwar</strong>,
                  <a href="https://scholar.google.com/citations?user=jtnM0c8AAAAJ&hl=en">Christian Laugier</a>

                  <br>
                  <em>IEEE International Conference on Advanced Robotics (ICAR), 2021
                  </em>
                  <br>
                  <a href="https://hal.inria.fr/hal-03416222/">pdf</a>
                  /
                  <a href="https://github.com/patilunmesh/collision_risk_estimation">code</a>
                  /
                  <a href="http://youtube.com/watch?v=CFsmtGfSajA">video</a>

                  <p>We propose new probabilistic models to obtain Stochastic Reachability Spaces for vehicles and
                    pedestrians detected in
                    the scene. We then exploit these probabilistic predictions of the road-users' future positions,
                    along with the expected
                    ego-vehicle trajectory, to estimate the probability of collision risk in real-time.</p>
                </td>
              </tr>









              <tr onmouseout="IV20_stop()" onmouseover="IV20_start()">
                <td style="padding:20px;width:30%;vertical-align:middle">
                  <div class="one">
                    <div class="two" id='IV20_video'><video width=100% height=100% muted autoplay loop>
                        <source src="images/IV20_video.mp4" type="video/mp4">
                        Your browser does not support the video tag.
                      </video></div>
                    <div id='IV20_still'><img src='images/IV20_still.png' width=100%></div>
                  </div>
                  <script type="text/javascript">
                    function IV20_start() {
                      document.getElementById('IV20_video').style.display = 'inline';
                      document.getElementById('IV20_still').style.display = 'none';
                    }

                    function IV20_stop() {
                      document.getElementById('IV20_video').style.display = 'none';
                      document.getElementById('IV20_still').style.display = 'inline';
                    }
                    IV20_stop()
                  </script>
                </td>


                <td style="padding:20px;width:70%;vertical-align:middle">
                  <a href="https://hal.inria.fr/hal-02696444/">
                    <papertitle>Probabilistic Collision Risk Estimation for Autonomous Driving: Validation via
                      Statistical Model Checking
                    </papertitle>
                  </a>
                  <br>
                  <strong>Anshul Paigwar</strong>, Eduard Baranov
                  <a href="https://sites.google.com/site/arenzaglia/">Alessandro Renzaglia</a>,
                  <a href="https://scholar.google.com/citations?user=jtnM0c8AAAAJ&hl=en">Christian Laugier</a>, Axel
                  Legay

                  <br>
                  <em>IEEE Intelligent Vehicles Symposium (IV), 2020
                  </em>
                  <br>
                  <a href="https://hal.inria.fr/hal-02696444/">pdf</a>
                  /
                  <a href="http://youtube.com/watch?v=-tJxAljXOmk&t">video</a>

                  <p>We present a Statistical Model Checking (SMC) approach to validate the collision risk assessment
                    generated by
                    a probabilistic perception system. SMC represents an intermediate between test and exhaustive
                    verification by relying on
                    statistics and evaluates the probability of (KPIs) based on a large number of simulations.</p>
                </td>
              </tr>








              <tr onmouseout="ICCVS21_stop()" onmouseover="ICCVS21_start()">
                <td style="padding:20px;width:30%;vertical-align:middle">
                  <div class="one">
                    <div class="two" id='ICCVS21_video'><video width=100% muted autoplay loop>
                        <source src="images/ICCVS21_video.mp4" type="video/mp4">
                        Your browser does not support the video tag.
                      </video></div>
                    <div id='ICCVS21_still'><img src='images/ICCVS21_still.png' width=100%></div>
                  </div>
                  <script type="text/javascript">
                    function ICCVS21_start() {
                      document.getElementById('ICCVS21_video').style.display = 'inline';
                      document.getElementById('ICCVS21_still').style.display = 'none';
                    }

                    function ICCVS21_stop() {
                      document.getElementById('ICCVS21_video').style.display = 'none';
                      document.getElementById('ICCVS21_still').style.display = 'inline';
                    }
                    ICCVS21_stop()
                  </script>
                </td>

                <td style="padding:20px;width:70%;vertical-align:middle">
                  <a href="https://hal.inria.fr/hal-03335282">
                    <papertitle>GridTrack: Detection and Tracking of Multiple Objects in Dynamic Occupancy Grids
                    </papertitle>
                    <!-- </a> -->
                    <br>
                    <a href="https://oerkent.gitlabpages.inria.fr/ozgurerkent/index.html">√ñzg√ºr Erkent</a>,
                    <a href="https://www.davidsgonzalez.com/">David Sierra-Gonzalez</a>,
                    <strong>Anshul Paigwar</strong>,
                    <a href="https://scholar.google.com/citations?user=jtnM0c8AAAAJ&hl=en">Christian Laugier</a>

                    <br>
                    <em>International Conference on Computer Vision Systems (ICVS), 2021 </em>
                    <br>
                    <a href="https://hal.inria.fr/hal-03335282">pdf</a>

                    <p> We fuse a dynamic occupancy grid map (DOGMa) with an object detector. DOGMa is obtained by
                      applying a Bayesian filter on
                      raw sensor data. This improves the tracking of the partially observed / unobserved objects with
                      the
                      help of the Bayesian
                      filter on raw data.</p>
                </td>




              <tr onmouseout="ICARCV20_stop()" onmouseover="ICARCV20_start()">
                <td style="padding:20px;width:30%;vertical-align:middle">
                  <div class="one">
                    <!-- <div id='ICARCV20_anim' class='two'>
                      <img src="images/ICARCV20_anim.gif" width=100%></a> -->
                    <div class="two" id='ICARCV20_anim'><video width=100% height=100% muted autoplay loop>
                        <source src="images/ICARCV20_video.mp4" type="video/mp4">
                        Your browser does not support the video tag.
                      </video></div>
                    <div id='ICARCV20_still'><img src="images/ICARCV20_still.png" width=100%></div>
                  </div>
                  <script type="text/javascript">
                    function ICARCV20_start() {
                      document.getElementById('ICARCV20_anim').style.display = 'inline';
                      document.getElementById('ICARCV20_still').style.display = 'none';
                    }

                    function ICARCV20_stop() {
                      document.getElementById('ICARCV20_anim').style.display = 'none';
                      document.getElementById('ICARCV20_still').style.display = 'inline';
                    }
                    ICARCV20_stop()
                  </script>
                </td>

                <td style="padding:20px;width:70%;vertical-align:middle">
                  <a href="https://hal.inria.fr/hal-03044979">
                    <papertitle>Leveraging Dynamic Occupancy Grids for 3D Object Detection in Point Clouds
                    </papertitle>
                    <!-- </a> -->
                    <br>
                    <a href="https://www.davidsgonzalez.com/">David Sierra-Gonzalez</a>,
                    <strong>Anshul Paigwar</strong>,
                    <a href="https://oerkent.gitlabpages.inria.fr/ozgurerkent/index.html">√ñzg√ºr Erkent</a>,
                    <a href="https://scholar.google.com/citations?user=jtnM0c8AAAAJ&hl=en">Christian Laugier</a>

                    <br>
                    <em>IEEE International Conference on Control, Automation, Robotics and Vision (ICARCV), 2020 </em>
                    <br>
                    <a href="https://hal.inria.fr/hal-03044979">pdf</a>
                    /
                    <a href="https://github.com/d451gon/kitti_dogma_dataset">code</a>
                    /
                    <a href="https://archive.org/details/kitti_dogma_dataset">dataset</a>

                    <!-- <a href="https://www.youtube.com/watch?v=2nrqhiiXJwY">results video</a> -->
                    <p></p>
                    <p> we use dynamic occupancy grid maps to exploit any dynamic information from the driving sequences
                      for 3D object detection.
                      Our results show that having access to the environment dynamics improves by 27% the ability of the
                      detection algorithm to predict the orientation of
                      smaller obstacles such as pedestrians.</p>
                </td>
              </tr>




              <tr onmouseout="ISMR21_stop()" onmouseover="ISMR21_start()">
                <td style="padding:20px;width:30%;vertical-align:middle">
                  <div class="one">
                    <div class="two" id='ISMR21_video'><video width=100% height=100% muted autoplay loop>
                        <source src="images/ISMR21_video.mp4" type="video/mp4">
                        Your browser does not support the video tag.
                      </video></div>
                    <div id='ISMR21_still'><img src='images/ISMR21_still.png' width=100%></div>
                  </div>
                  <script type="text/javascript">
                    function ISMR21_start() {
                      document.getElementById('ISMR21_video').style.display = 'inline';
                      document.getElementById('ISMR21_still').style.display = 'none';
                    }

                    function ISMR21_stop() {
                      document.getElementById('ISMR21_video').style.display = 'none';
                      document.getElementById('ISMR21_still').style.display = 'inline';
                    }
                    ISMR21_stop()
                  </script>
                </td>


                <td style="padding:20px;width:70%;vertical-align:middle">
                  <a href="https://hal.archives-ouvertes.fr/hal-03476539/">
                    <papertitle>Sahayak-An Autonomous COVID Aid Bot
                    </papertitle>
                  </a>
                  <br>
                  Karthik Raman, Prathamesh Ringe, Sania Subhedar, Sushlok Shah, Yagnesh Devada, Aayush Fadia, Kushagra
                  Srivastava, Varad
                  Vaidya,
                  <a href="https://harshadzade.github.io/">Harshad Zade</a>,
                  <a href="https://www.ajinkyakamat.com/home">Ajinkya Kamat</a>,
                  <strong>Anshul Paigwar</strong>,
                  <a href="https://scholar.google.co.in/citations?user=B9InqKQAAAAJ&hl=en">Shital Chiddarwar</a>
                  <br>
                  <em>IEEE International Symposium on Medical Robotics (ISMR), 2021, poster submission
                  </em>
                  <br>
                  <a href="https://hal.archives-ouvertes.fr/hal-03476539/">pdf</a>
                  /
                  <a href="https://www.aidbots.in/#/">project page</a>
                  /
                  <a href="https://youtu.be/pSqHcjDWims">video</a>

                  <p>We propose a general framework for developing medical assistive robots capable of delivering food
                    and medicine
                    to patients and facilitating teleconferencing with doctors. Autonomous mobility of robot is
                    validated in
                    simulated environments, while a teleoperated prototype was deployed at AIIMS Nagpur, India.</p>
                </td>
              </tr>


              <tr onmouseout="IV17_stop()" onmouseover="IV17_start()">
                <td style="padding:20px;width:30%;vertical-align:middle">
                  <div class="one">
                    <div class="two" id='IV17_video'><video width=100% height=100% muted autoplay loop>
                        <source src="images/IV17_video.mp4" type="video/mp4">
                        Your browser does not support the video tag.
                      </video></div>
                    <div id='IV17_still'><img src='images/IV17_still.png' width=100%></div>
                  </div>
                  <script type="text/javascript">
                    function IV17_start() {
                      document.getElementById('IV17_video').style.display = 'none';
                      document.getElementById('IV17_still').style.display = 'inline';
                    }

                    function IV17_stop() {
                      document.getElementById('IV17_video').style.display = 'none';
                      document.getElementById('IV17_still').style.display = 'inline';
                    }
                    IV17_stop()
                  </script>
                </td>


                <td style="padding:20px;width:70%;vertical-align:middle">
                  <a href="https://hal.inria.fr/hal-01579095/">
                    <papertitle>Ground estimation and point cloud segmentation using spatiotemporal conditional random
                      field
                    </papertitle>
                  </a>
                  <br>
                  Lukas Rummelhard, <strong>Anshul Paigwar</strong>,
                  <a href="https://scholar.google.com/citations?user=IhpXewIAAAAJ&hl=en">Amaury Negre</a>,
                  <a href="https://scholar.google.com/citations?user=jtnM0c8AAAAJ&hl=en">Christian Laugier</a>

                  <br>
                  <em>IEEE Intelligent Vehicles Symposium (IV), 2017
                  </em>
                  <br>
                  <a href="https://hal.inria.fr/hal-01579095/">pdf</a>
                  /
                  <a href="https://youtu.be/4SMFlB9yg0I">video</a>

                  <p>We propose a Spatio-Temporal Conditional Random Field (STCRF) method for ground modeling and
                    labeling
                    in 3D Point clouds, based on a local ground elevation estimation. Spatial and
                    temporal dependencies within the segmentation process are unified by a dynamic probabilistic.</p>
                </td>
              </tr>











              <tr onmouseout="ICM17_stop()" onmouseover="ICM17_start()">
                <td style="padding:20px;width:30%;vertical-align:middle">
                  <div class="one">
                    <div class="two" id='ICM17_video'><video width=100% height=100% muted autoplay loop>
                        <source src="images/ICM17_video.mp4" type="video/mp4">
                        Your browser does not support the video tag.
                      </video></div>
                    <div id='ICM17_still'><img src='images/ICM17_still.png' width=100%></div>
                  </div>
                  <script type="text/javascript">
                    function ICM17_start() {
                      document.getElementById('ICM17_video').style.display = 'inline';
                      document.getElementById('ICM17_still').style.display = 'none';
                    }

                    function ICM17_stop() {
                      document.getElementById('ICM17_video').style.display = 'none';
                      document.getElementById('ICM17_still').style.display = 'inline';
                    }
                    ICM17_stop()
                  </script>
                </td>


                <td style="padding:20px;width:70%;vertical-align:middle">
                  <a
                    href="https://www.researchgate.net/profile/Akash-Singh-15/publication/325473233_Design_and_Implementation_of_Omnidirectional_Spherical_Modular_snake_robotOSMOS/links/5b1006c84585150a0a5c982d/Design-and-Implementation-of-Omnidirectional-Spherical-Modular-snake-robotOSMOS.pdf">
                    <papertitle>Design and implementation of Omni-directional spherical modular snake robot (OSMOS)
                    </papertitle>
                  </a>
                  <br>
                  <a href="https://scholar.google.it/citations?user=-AbpUDEAAAAJ&hl=en">Akash Singh</a>,
                  <strong>Anshul Paigwar</strong>,
                  <a href="https://in.linkedin.com/in/saiteja-manchukanti">Sai Teja Manchukanti</a>,
                  <a href="https://prasadvagdargi.github.io/">Prasad Vagdargi</a>,
                  <a href="https://www.linkedin.com/in/manish-the-maker">Manish Maurya</a>,
                  <a href="https://sites.google.com/view/manishsaroya">Manish Saroya</a>,
                  <a href="https://scholar.google.co.in/citations?user=B9InqKQAAAAJ&hl=en">Shital Chiddarwar</a>

                  <br>
                  <em>IEEE International Conference on Mechatronics (ICM), 2017
                    <br>
                    International Conference on Reconfigurable Mechanisms and Robots (ReMAR), 2018
                  </em>
                  <br>
                  <a
                    href="https://www.researchgate.net/profile/Akash-Singh-15/publication/325473233_Design_and_Implementation_of_Omnidirectional_Spherical_Modular_snake_robotOSMOS/links/5b1006c84585150a0a5c982d/Design-and-Implementation-of-Omnidirectional-Spherical-Modular-snake-robotOSMOS.pdf">pdf</a>
                  /
                  <a
                    href="https://www.researchgate.net/profile/Manish-Saroya-2/publication/325473226_Design_and_Motion_Analysis_of_Complicant_Omni-directional_Spherical_Modular_Snake_Robot_COSMOS/links/5bf1d8674585150b2bc116f4/Design-and-Motion-Analysis-of-Complicant-Omni-directional-Spherical-Modular-Snake-Robot-COSMOS.pdf">pdf-2</a>
                  /
                  <a href="https://youtu.be/1vV_QFoUCyk">video</a>

                  <p>We presents a novel design of a Compliant Omni-directional snake robot
                    (COSMOS) consisting of mechanically and software linked spherical robot modules.</p>
                </td>
              </tr>

            </tbody>
          </table>








          <!-- Other Projects -->















          <table
            style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;">
            <tbody>
              <tr>
                <td style="padding:20px;width:100%;vertical-align:middle">
                  <heading>Other Projects</heading>
                  <p>
                    I have always been fascinated by robots; building them and bringing them to life is something I take
                    pleasure in.
                    List of my other projects can be found <a
                      href="http://anshulpaigwar.weebly.com/projects.html">here</a>.
                  </p>
                </td>
              </tr>
            </tbody>
          </table>
          <table
            style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;">
            <tbody>



              <tr onmouseout="PW_Conv_stop()" onmouseover="PW_Conv_start()">
                <td style="padding:20px;width:30%;vertical-align:middle">
                  <div class="one">
                    <div class="two" id='PW-Conv_video'><video width=100% height=100% muted autoplay loop>
                        <source src="images/PW-Conv_video.mp4" type="video/mp4">
                        Your browser does not support the video tag.
                      </video></div>
                    <div id='PW-Conv_still'><img src='images/PW-Conv_still.png' width=100%></div>
                  </div>
                  <script type="text/javascript">
                    function PW_Conv_start() {
                      document.getElementById('PW-Conv_video').style.display = 'inline';
                      document.getElementById('PW-Conv_still').style.display = 'none';
                    }

                    function PW_Conv_stop() {
                      document.getElementById('PW-Conv_video').style.display = 'none';
                      document.getElementById('PW-Conv_still').style.display = 'inline';
                    }
                    PW_Conv_stop()
                  </script>
                </td>


                <td style="padding:20px;width:70%;vertical-align:middle">

                  <papertitle> PyTorch implementation of Pointwise Convolutional Neural Networks
                  </papertitle>

                  <br>
                  <a href="https://github.com/anshulpaigwar/Pointwise-Convolutional-Neural-Network">code</a>
                  /
                  <a href="https://arxiv.org/pdf/1712.05245.pdf">original paper</a>
                  /
                  <a href="https://youtu.be/Q00o88Q_08w">video</a>

                  <p> A new pointwise convolution operator that can be applied at each point of a 3D point cloud.
                    can yield competitive accuracy in both semantic segmentation and object recognition task.
                  </p>
                </td>
              </tr>




              <tr onmouseout="datmo_stop()" onmouseover="datmo_start()">
                <td style="padding:20px;width:30%;vertical-align:middle">
                  <div class="one">
                    <div class="two" id='datmo_video'><video width=100% height=100% muted autoplay loop>
                        <source src="images/datmo_video.mp4" type="video/mp4">
                        Your browser does not support the video tag.
                      </video></div>
                    <div id='datmo_still'><img src='images/datmo_still.png' width=100%></div>
                  </div>
                  <script type="text/javascript">
                    function datmo_start() {
                      document.getElementById('datmo_video').style.display = 'inline';
                      document.getElementById('datmo_still').style.display = 'none';
                    }

                    function datmo_stop() {
                      document.getElementById('datmo_video').style.display = 'none';
                      document.getElementById('datmo_still').style.display = 'inline';
                    }
                    datmo_stop()
                  </script>
                </td>


                <td style="padding:20px;width:70%;vertical-align:middle">

                  <papertitle> Dynamic object detection and tracking in point clouds
                  </papertitle>
                  <br>
                  <strong>Anshul Paigwar</strong>,
                  <a href="https://www.linkedin.com/in/zubin-priyansh/?originalSubdomain=in">Zubin Priyansh</a>,
                  <a href="https://www.linkedin.com/in/pradyotkvn/?originalSubdomain=in">Pradyot KVN</a>
                  <br>
                  <em>Internship at Hitech Robotics systems, Gurgaon, India </em>
                  <br>
                  <a href="https://github.com/anshulpaigwar/dynamic-object-tracking-pointcloud">code</a>
                  /
                  <a
                    href="https://anshulpaigwar.github.io/dynamic-object-tracking-pointcloud/classdatmo_1_1cloud__segmentation.html">documentation</a>
                  /
                  <a href="https://drive.google.com/open?id=0B1AaW7rsinFEOEZ4QVdXSWFDX1U">video</a>
                  <p> We proposed a new framework for real-time Detection and Tracking for Moving Objects (DATMO) in 3D
                    point clouds.
                    We tested our algorithm with the point cloud from Intel Realsense Camera and Velodyne-16 in indoor
                    environment.
                  </p>

                </td>
              </tr>








              <tr onmouseout="coms_stop()" onmouseover="coms_start()">
                <td style="padding:20px;width:30%;vertical-align:middle">
                  <div class="one">
                    <div class="two" id='coms_video'><video width=100% height=100% muted autoplay loop>
                        <source src="images/coms_video.mp4" type="video/mp4">
                        Your browser does not support the video tag.
                      </video></div>
                    <div id='coms_still'><img src='images/coms_still.jpg' width=100%></div>
                  </div>
                  <script type="text/javascript">
                    function coms_start() {
                      document.getElementById('coms_video').style.display = 'inline';
                      document.getElementById('coms_still').style.display = 'none';
                    }

                    function coms_stop() {
                      document.getElementById('coms_video').style.display = 'none';
                      document.getElementById('coms_still').style.display = 'inline';
                    }
                    coms_stop()
                  </script>
                </td>









                <td style="padding:20px;width:70%;vertical-align:middle">

                  <papertitle> EKF based sensor fusion for localization of autonomous vehicles
                  </papertitle>
                  <br>
                  <strong>Anshul Paigwar</strong>,
                  <a href="https://www.linkedin.com/in/anthonywongsg/?originalSubdomain=sg">Anthony Wong</a>
                  <br>
                  <em>Internship at Institute of Infocom Research, Singapore </em>
                  <br>
                  <a href="https://drive.google.com/open?id=0B1AaW7rsinFEa19YSl9yYlQ2Nm8">video</a>

                  <p> We fused GPS, odometry and IMU data for accurate localisation of the vehicle.
                    We used Constant Heading and Velocity (CHCV) Vehicle Model for the dynamics of the vehicle.
                    Finally the sensor fusion algorithm was tested on Toyota E-COMs experimental autonomous vehicle
                    platform.
                  </p>
                </td>
              </tr>







              <tr onmouseout="road_curb_stop()" onmouseover="road_curb_start()">
                <td style="padding:20px;width:30%;vertical-align:middle">
                  <div class="one">
                    <div class="two" id='road_curb_video'><video width=100% height=100% muted autoplay loop>
                        <source src="images/road_curb_video.mp4" type="video/mp4">
                        Your browser does not support the video tag.
                      </video></div>
                    <div id='road_curb_still'><img src='images/road_curb_still.png' width=100%></div>
                  </div>
                  <script type="text/javascript">
                    function road_curb_start() {
                      document.getElementById('road_curb_video').style.display = 'inline';
                      document.getElementById('road_curb_still').style.display = 'none';
                    }

                    function road_curb_stop() {
                      document.getElementById('road_curb_video').style.display = 'none';
                      document.getElementById('road_curb_still').style.display = 'inline';
                    }
                    road_curb_stop()
                  </script>
                </td>









                <td style="padding:20px;width:70%;vertical-align:middle">

                  <papertitle> Road boundaries detection for autonomous vehicle navigation
                  </papertitle>
                  <br>
                  <strong>Anshul Paigwar</strong>,
                  <a href="https://www.linkedin.com/in/anthonywongsg/?originalSubdomain=sg">Anthony Wong</a>
                  <br>
                  <em>Internship at Institute of Infocom Research, Singapore </em>
                  <br>
                  <a href="https://drive.google.com/file/d/0B1AaW7rsinFETjFZR3l3UmRDUDg/view?usp=sharing">video</a>

                  <p> For detection in 3D point cloud we first use RANSAC algorithm to find a dominant plane (road).
                    Then use region growing
                    algorithm and orientation of normal to road plane to detect the edges of the road. Finally we fit a
                    cubic spline through the detected points.
                  </p>
                </td>
              </tr>




            </tbody>
          </table>










          <table
            style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;">
            <tbody>
              <tr>
                <td style="padding:0px">
                  <br>
                  <p style="text-align:right;font-size:small;">
                    This website was forked from <a href="https://github.com/jonbarron/jonbarron_website">source
                      code</a>

                  </p>
                </td>
              </tr>
            </tbody>
          </table>
        </td>
      </tr>
  </table>
</body>

</html>




<!-- <tr onmouseout="hnerf_stop()" onmouseover="hnerf_start()">
            <td style="padding:20px;width:25%;vertical-align:middle">
              <div class="one">
                <div class="two" id='hnerf_image'><video width=100% height=100% muted autoplay loop>
                    <source src="images/hnerf_after.mp4" type="video/mp4">
                    Your browser does not support the video tag.
                  </video></div>
                <img src='images/hnerf_before.jpg' width="160">
              </div>
              <script type="text/javascript">
                function hnerf_start() {
                  document.getElementById('hnerf_image').style.opacity = "1";
                }

                function hnerf_stop() {
                  document.getElementById('hnerf_image').style.opacity = "0";
                }
                hnerf_stop()
              </script>
            </td>
            <td style="padding:20px;width:75%;vertical-align:middle">
              <a href="https://grail.cs.washington.edu/projects/humannerf/">
                <papertitle>HumanNeRF: Free-viewpoint Rendering of Moving People from Monocular Video</papertitle>
              </a>
              <br>
              <a href="https://homes.cs.washington.edu/~chungyi/">Chung-Yi Weng</a>,
              <a href="https://homes.cs.washington.edu/~curless/">Brian Curless</a>,
              <a href="https://pratulsrinivasan.github.io/">Pratul Srinivasan</a>, <br>
              <strong>Jonathan T. Barron</strong>,
              <a href="https://www.irakemelmacher.com/">Ira Kemelmacher-Shlizerman </a>
              <br>
              <em>CVPR</em>, 2022
              <br>
              <a href="https://grail.cs.washington.edu/projects/humannerf/">project page</a>
              /
              <a href="https://arxiv.org/abs/2201.04127">arXiv</a>
              /
              <a href="https://youtu.be/GM-RoZEymmw">video</a>
              <p></p>
              <p>Combining NeRF with pose estimation lets you use a monocular video to do free-viewpoint rendering of a human.</p>
            </td>
          </tr> -->



<!-- <tr onmouseout="cvpr12_stop()" onmouseover="cvpr12_start()">
            <td style="padding:20px;width:40%;vertical-align:middle">
              <div class="one" style="height: 120px">
                <div class="two" id='cvpr12_image' style="height: 120px">
                  <img src='images/frustrum-pointpillars.png'   width=100% style="border-style: none">
                </div>
                <img src='images/frustrum-pointpillars.png' width= 100% style="border-style: none">
              </div>
              <script type="text/javascript">
                function cvpr12_start() {
                  document.getElementById('cvpr12_image').style.opacity = "1";
                }

                function cvpr12_stop() {
                  document.getElementById('cvpr12_image').style.opacity = "0";
                }
                cvpr12_stop()
              </script>
            </td>
            <td width="60%" valign="middle">
              <a href="https://drive.google.com/file/d/17RfINbE2dr2EjXp9MtGO0MHJLQmQVhvT/view?usp=sharing">
                <papertitle>Shape, Albedo, and Illumination from a Single Image of an Unknown Object</papertitle>
              </a>
              <br>
              <strong>Jonathan T. Barron</strong>, <a href="http://www.cs.berkeley.edu/~malik/">Jitendra Malik</a>
              <br>
              <em>CVPR</em>, 2012
              <br>
              <a href="https://drive.google.com/file/d/1Im_bUI42AP9VPoNtsjLajvtLRiwv39k3/view?usp=sharing">supplement</a> /
              <a href="data/BarronMalikCVPR2012.bib">bibtex</a> /
              <a href="https://drive.google.com/file/d/1IAlSF4k3_CEL9dfbaMiNTFPBoEkLhsRl/view?usp=sharing">poster</a>
              <p>This paper is subsumed by <a href="#SIRFS">SIRFS</a>.</p>
            </td>
          </tr> -->



<!-- 				
        <table width="100%" align="center" border="0" cellspacing="0" cellpadding="20"><tbody>
          <tr>
            <td>
              <heading>Misc</heading>
            </td>
          </tr>
        </tbody></table>
        <table width="100%" align="center" border="0" cellpadding="20"><tbody>
					
          <tr>
            <td style="padding:20px;width:25%;vertical-align:middle"><img src="images/cvf.jpg"></td>
            <td width="75%" valign="center">
              <a href="https://cvpr2022.thecvf.com/area-chairs">Area Chair, CVPR 2022</a>
              <br>
              <a href="http://cvpr2021.thecvf.com/area-chairs">Area Chair & Longuet-Higgins Award Committee Member, CVPR 2021</a>
              <br>
              <a href="http://cvpr2019.thecvf.com/area_chairs">Area Chair, CVPR 2019</a>
              <br>
              <a href="http://cvpr2018.thecvf.com/organizers/area_chairs">Area Chair, CVPR 2018</a>
            </td>
          </tr>
          <tr>
            <td style="padding:20px;width:25%;vertical-align:middle">
              <img src="images/cs188.jpg" alt="cs188">
            </td>
            <td width="75%" valign="center">
              <a href="http://inst.eecs.berkeley.edu/~cs188/sp11/announcements.html">Graduate Student Instructor, CS188 Spring 2011</a>
              <br>
              <a href="http://inst.eecs.berkeley.edu/~cs188/fa10/announcements.html">Graduate Student Instructor, CS188 Fall 2010</a>
              <br>
              <a href="http://aima.cs.berkeley.edu/">Figures, "Artificial Intelligence: A Modern Approach", 3rd Edition</a>
            </td>
          </tr>
					

          <tr>
            <td align="center" style="padding:20px;width:25%;vertical-align:middle">
							<heading>Basically <br> Blog Posts</heading>
            </td>
            <td width="75%" valign="middle">
              <a href="https://arxiv.org/abs/2112.11687">Squareplus: A Softplus-Like Algebraic Rectifier</a>
              <br>
              <a href="https://arxiv.org/abs/2010.09714">A Convenient Generalization of Schlick's Bias and Gain Functions</a>
              <br>
              <a href="https://arxiv.org/abs/1704.07483">Continuously Differentiable Exponential Linear Units</a>
            </td>
          </tr> -->
